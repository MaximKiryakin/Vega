





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
plt.style.use('ggplot')
import seaborn as sns

from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
import pmdarima as pm

from sklearn.preprocessing import LabelEncoder
from xgboost import XGBRegressor

from hyperopt.fmin import fmin
from hyperopt.pyll import scope
from hyperopt import hp, tpe, Trials, STATUS_OK

from sklearn.model_selection import TimeSeriesSplit
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import train_test_split

import pickle
from Utils.graphs import *

graphsDir = "./Plots"
UseBackUpData = False








labels = {
    "store_nbr":    "id магазина",
    "sales":        "Число продаж",
    "onpromotion":  "Число акций",
    "dcoilwtico":   "Цена топлива",
    "transactions": "Число транзакций",
    "city":         "Страна",
    "cluster":      "Группа"
}


data = [
    "holidays_events.csv",
    "oil.csv",
    "stores.csv",
    "train.csv",
    "transactions.csv"
]

leftDate, rightDate = '2016-01-01', '2017-01-01'
dataframes = {}

for file in data:
    name = file.split(".")[0]
    dataframes[name] = pd.read_csv("Data/" + file)

dataframes["trainTotal"] = (
    dataframes["train"]
    .merge(dataframes["holidays_events"].drop_duplicates("date"), on="date", how="left")
    .merge(dataframes["oil"].drop_duplicates("date"), on="date", how="left")
    .merge(dataframes["transactions"], on=["date", "store_nbr"], how="left")
    .merge(dataframes["stores"].drop_duplicates("store_nbr"), on="store_nbr", how="left")
    .rename(columns = {'type_x' : "holiday_type", "type_y" : "store_type"})
)
assert dataframes["trainTotal"].shape[0] == dataframes["train"].shape[0]


timeBasedFeatures = ["dayofweek", "quarter", "month", "year",
                     "dayofyear", "day", "week", "lag1", "lag2", "lag3"]

dataframes["trainTotal"] = (
    dataframes["trainTotal"]
    .assign(date=pd.to_datetime(dataframes["trainTotal"]["date"], format='%Y-%m-%d'))
    .assign(dayofweek=lambda df: df['date'].dt.dayofweek)
    .assign(quarter=lambda df: df['date'].dt.quarter)
    .assign(month=lambda df: df['date'].dt.month)
    .assign(year=lambda df: df['date'].dt.year)
    .assign(dayofyear=lambda df: df['date'].dt.dayofyear)
    .assign(day=lambda df: df['date'].dt.day)
    .assign(week=lambda df: df['date'].dt.isocalendar().week)
)

dataframes["trainTotal"] = (
    dataframes["trainTotal"]
    .astype({"week": np.int32, "day": np.int32, "dayofyear": np.int32, "year": np.int32,
             "month": np.int32, "quarter": np.int32, "dayofweek": np.int32})
)


dataframes["trainTotal"] = dataframes["trainTotal"].set_index("date")

target_map = dataframes["trainTotal"]["sales"].to_dict()
dataframes["trainTotal"] = (
    dataframes["trainTotal"]
    .assign(lag1 = (dataframes["trainTotal"].index - pd.Timedelta("364 days")).map(target_map))
    .assign(lag2 = (dataframes["trainTotal"].index - pd.Timedelta("728 days")).map(target_map))
    .assign(lag3 = (dataframes["trainTotal"].index - pd.Timedelta("1092 days")).map(target_map))
)
dataframes["trainTotal"] = (
    dataframes["trainTotal"]
    .fillna({"lag1": dataframes["trainTotal"]["lag1"].mean()})
    .fillna({"lag2": dataframes["trainTotal"]["lag2"].mean()})
    .fillna({"lag3": dataframes["trainTotal"]["lag3"].mean()})
)

dataframes["trainTotal"] = dataframes["trainTotal"].reset_index()


dataframes["trainTotal"].head(5)


specification = {}
specification.update({"columns" : dataframes["trainTotal"].columns})
with open("Spec/specification.pkl", "wb") as f:
    pickle.dump(specification, f)


data = (
    dataframes["trainTotal"][["date", "dcoilwtico"]]
    .set_index("date")
    .rename(columns={"dcoilwtico": "Цена на топливо"})
    .reset_index()
)

plotTimeSeries(data=data, X="date", y="Цена на топливо", xlabel="Дата", color="blue",
               title="Цена на топливо в Эквадоре за все время", ylabel="Цена (усл. ед)",
               linewidth=0.8, savePath=f"{graphsDir}/OilPrice.png");


dataframes["trainTotal"] = (
    dataframes["trainTotal"]
    .assign(dcoilwtico=dataframes["trainTotal"]['dcoilwtico'].interpolate(method='polynomial', order=2))
    .assign(transactions=dataframes["trainTotal"]['transactions'].fillna(dataframes["trainTotal"]['transactions'].mean()))
)


data = (
    dataframes["trainTotal"][["date", "dcoilwtico"]]
    .set_index("date")
    .rename(columns={"dcoilwtico": "Цена на топливо"})
    .reset_index()
)

plotTimeSeries(data=data, X="date", y="Цена на топливо", xlabel="Дата", color="blue",
               title="Цена на топливо в Эквадоре за все время (интерполяция)", ylabel="Цена (усл. ед)",
               linewidth=0.8, savePath=f"{graphsDir}/OilPriceEnterpolated.png");


dataframes["trainTotal"] = dataframes["trainTotal"].query("(date >= @leftDate) & (date < @rightDate)")


size = dataframes["trainTotal"].shape[0]
(
    pd.DataFrame(dataframes["trainTotal"].isna().sum()).reset_index().rename(columns={"index":"column", 0: "missing"})
    .eval("share=missing/@size")
    .query("missing > 0")
)


dataframes["trainTotal"] = (
    dataframes["trainTotal"]
    .drop(columns=["holiday_type", "locale", "locale_name", "description", "transferred"])
)


size = dataframes["trainTotal"].shape[0]
tmp = (
    pd.DataFrame(dataframes["trainTotal"].isna().sum()).reset_index().rename(columns={"index":"column", 0: "missing"})
    .eval("share=missing/@size")
    .query("missing > 0")
)
assert tmp.shape[0] == 0


fig, ax = plt.subplots(figsize=(9, 9))
numColsToDrop = list(dataframes["trainTotal"].select_dtypes(exclude=[np.number]).columns) + ["id", "year"]
data = dataframes["trainTotal"].drop(columns=numColsToDrop).rename(columns=labels)

corr = data.corr()

sns.heatmap(corr, cmap="coolwarm", ax = ax)
ax.set_title("Матрица корреляций вещественных признаков")
plt.savefig(f"{graphsDir}/CorrMatrix.png", bbox_inches='tight'),
plt.tight_layout(), plt.show();


plt.figure(figsize=(10, 4))
data = (
    dataframes["trainTotal"]
    .groupby('family')['sales']
    .sum()
    .reset_index()
    .sort_values(by='sales', ascending=False)
)

plt.bar(data['family'], data['sales'], color='red')

plt.xticks(rotation=90, ha='right')

plt.title('Число продаж по категориям')
plt.xlabel('Категория')
plt.ylabel('Число продаж')
plt.savefig(f"{graphsDir}/CategoriesDistribution.png", bbox_inches='tight');


dataframes["trainTotal"].head(5)


catColumns = ["family","store_nbr", "city", "state", "store_type", "cluster"]

for cat in catColumns:
    label_encoder = LabelEncoder()
    dataframes["trainTotal"][cat] = label_encoder.fit_transform(dataframes["trainTotal"][cat])


dataframes["trainTotal"].head(5)


mode_function = lambda x: x.mode()[0]
mean_columns  = ["sales", "onpromotion", "dcoilwtico", "transactions"]
mode_columns  = [
    "store_nbr", "family", "city", "state", "store_type", "cluster",
    "lag1", "lag2", "lag3", "dayofweek", "quarter", "month", "year",
    "dayofyear", "day", "week"
]

agg_dict = {i: mode_function for i in mode_columns}
agg_dict.update({i:"mean"  for i in mean_columns})

trainDF = dataframes["trainTotal"].groupby("date").agg(agg_dict).reset_index()
trainDF.head(5)


noStdCols = []
for col in trainDF.columns[1:]:
    if not np.std(trainDF[col]):
        noStdCols.append(col)

trainDF.drop(columns=noStdCols, inplace=True)


timeBasedFeatures = list(set(timeBasedFeatures).intersection(set(trainDF.columns)))


trainDF.head(5)


plotTimeSeries(data=trainDF, X="date", y="sales", xlabel="Дата", ylabel="Число продаж",
               title="Число продаж в Эквадоре по датам за 2016 год", color="blue",
               linewidth=0.8, savePath=f"{graphsDir}/Sales2016.png");





plotAcfPacf(data=trainDF, y="sales", savePath=f"{graphsDir}/AcfPacf.png")


adf_test = adfuller(trainDF["sales"])
print(f"p-value: {adf_test[1]}")


trainDF.head(5)





trainDF_new = trainDF.set_index("date").diff().fillna(0).reset_index()


trainDF_new


plotAcfPacf(data=trainDF_new, y="sales", savePath=f"{graphsDir}/AcfPacf.png")


adf_test = adfuller(trainDF_new["sales"])
print(f"p-value: {adf_test[1]}")





train_size = int(trainDF["sales"].size * 0.8)
test_size  = trainDF["sales"].size - train_size


fig, ax = plt.subplots(1, 1, figsize=(15, 3))

trainDF["forecast_naive"] = [None]*train_size + [trainDF.loc[train_size - train_size*0.8:train_size, "sales"].mean()] * test_size
trainDF["prediction"] = [0] * train_size + [1] * test_size
trainDF_new["prediction"] = [0] * train_size + [1] * test_size

ax = plotTimeSeries(data=trainDF, X="date", y="sales", axs=ax, xlabel="Дата",
                    ylabel="Количество продаж", color="blue", label="Факт",
                    linewidth=0.8, title="Число продаж в Эквадоре по датам за 2016 год (наивный прогноз)")

ax = plotTimeSeries(data=trainDF, X="date", y="forecast_naive", axs=ax, xlabel="Дата",
                    ylabel="Количество продаж", color="red", label="Прогноз",
                    linestyle="--", title="Число продаж в Эквадоре по датам за 2016 год (наивный прогноз)",
                    savePath=f"{graphsDir}/SalesNaive.png")





model = ARIMA(trainDF_new.loc[:train_size, "sales"], order=(2, 1, 0))
model_fit = model.fit()
print(model_fit.summary())


irf = model_fit.impulse_responses(20, orthogonalized=True, impulse=[1]).plot(figsize=(15,3), title="IRF для ARIMA модели")


fig, ax = plt.subplots(1, 2, figsize=(15, 3))

residuals = model_fit.resid[1:]

residuals.plot(title="Residuals", ax=ax[0], color="blue")
residuals.plot(title="Density", kind="kde", ax=ax[1], color="blue")
plt.savefig(f"{graphsDir}/ResidualsDensity.png", bbox_inches='tight')


fig, ax = plt.subplots(1, 1, figsize=(15, 3))

forecast_test = model_fit.forecast(test_size)
trainDF_new["forecast_manual"] = [None]*train_size + list(forecast_test)

ax = plotTimeSeries(data=trainDF_new, X="date", y="sales", axs=ax, linewidth=0.8, color="blue")

ax = plotTimeSeries(data=trainDF_new, X="date", y="forecast_manual", axs=ax, xlabel="Дата",
                    ylabel="Количество продаж", color="red", 
                    title="Число продаж в Эквадоре по датам за 2016 год (ARIMA)", label="Прогноз",
                    linestyle="--", savePath=f"{graphsDir}/SalesARIMA.png")


trainDF.head(5)


train_size = int(trainDF.shape[0] * 0.9)
test_size  = trainDF.shape[0] - train_size

cols = ["onpromotion", "dcoilwtico", "transactions"]
X_train_timebased = trainDF.loc[:train_size, timeBasedFeatures]
X_test_timebased  = trainDF.loc[train_size:, timeBasedFeatures]

X_train_all = trainDF.loc[:train_size, timeBasedFeatures + cols]
X_test_all  = trainDF.loc[train_size:, timeBasedFeatures + cols]

y_train = trainDF.loc[:train_size, "sales"]
y_test  = trainDF.loc[train_size:, "sales"]


model_timebased = SARIMAX(endog=y_train, exog=X_train_timebased).fit()
forecast_timebased = model_timebased.forecast(exog=X_test_timebased, steps=X_test_timebased.shape[0])

model_all = SARIMAX(endog=y_train, exog=X_train_all).fit()
forecast_all = model_all.forecast(exog=X_test_all, steps=X_test_all.shape[0])


trainDF["forecast_SARIMAX_timebased"] = [None]*train_size + list(forecast_timebased.values)
trainDF["forecast_SARIMAX_all"] = [None]*train_size + list(forecast_all.values)


fig, ax = plt.subplots(1, 1, figsize=(15, 3))

ax = plotTimeSeries(data=trainDF, X="date", y="sales", axs=ax, xlabel="Дата",
                    ylabel="Количество продаж", color="blue", title="", label="Факт",
                    linewidth=0.8)

ax = plotTimeSeries(data=trainDF, X="date", y="forecast_SARIMAX_timebased", axs=ax, 
                    color="red", label="Прогноз (Временные переменные)", linestyle="--")

ax = plotTimeSeries(data=trainDF, X="date", y="forecast_SARIMAX_all", axs=ax, xlabel="Дата",
                    ylabel="Количество продаж", color="green", label="Прогноз (Все переменные)",
                    title="Число продаж в Эквадоре по датам за 2016 год (SARIMAX)",
                    linestyle="--", savePath=f"{graphsDir}/SalesSARIMAX.png")





def findBestParams(X_train: pd.DataFrame, y_train: pd.DataFrame):
    space = {
        'n_estimators':  scope.int(hp.quniform('n_estimators', 3000, 7000, 500)),
        'max_depth':     scope.int(hp.quniform('max_depth', 1, 10, 1)),
        'learning_rate': hp.loguniform('learning_rate', np.log(0.00001), np.log(1))
    }
    
    def objective(params):
        model = XGBRegressor(**params)
        split = TimeSeriesSplit(n_splits=10)
        score = cross_val_score(model, X_train, y_train, cv=split,
                                scoring='neg_mean_absolute_error').mean()
    
        return {'loss': -1*score, 'params': params, 'status': STATUS_OK}
    
    best_params = fmin(fn=objective, space=space, algo=tpe.suggest,
                       max_evals=60, trials=Trials(), verbose=1)
    
    return best_params


def getOptimalPrediction(X_train:pd.DataFrame, X_test:pd.DataFrame, y_train:pd.DataFrame, model):

    best_params = findBestParams(X_train=X_train_timebased, y_train=y_train)
    best_params.update({"max_depth": int(best_params["max_depth"]),
                        "n_estimators": int(best_params["n_estimators"])})

    model_inner = model(**best_params)
    model_inner.fit(X_train, y_train)
    prediction = model_inner.predict(X_test)

    return [best_params, prediction]


# ячейка выполняется долго. Лучше просто загрузить предрасчет
with open("Spec/specification.pkl", "rb") as f:
    spec = pickle.load(f)

if not UseBackUpData:
    coeffs, predictionTimebased = getOptimalPrediction(X_train=X_train_timebased, X_test=X_test_timebased, 
                                                       y_train=y_train, model=XGBRegressor)
    spec.update({"predictionTimebasedCoeffs": coeffs, "predictionTimebased": predictionTimebased})

    coeffs, predictionAll = getOptimalPrediction(X_train=X_train_all, X_test=X_test_all,
                                                 y_train=y_train, model=XGBRegressor)
    spec.update({"predictionAllCoeffs": coeffs, "predictionAll": predictionAll})
    
    with open("Spec/specification.pkl", "wb") as f:
        pickle.dump(spec, f)


with open("Spec/specification.pkl", "rb") as f:
    spec = pickle.load(f)


trainDF["pr_boosting_timebased"] = [None]*train_size + list(spec["predictionTimebased"])
trainDF["pr_boosting_all"] = [None]*train_size + list(spec["predictionAll"])


fig, ax = plt.subplots(1, 1, figsize=(15, 3))

ax = plotTimeSeries(data=trainDF, X="date", y="sales", axs=ax, color="blue", label="Факт", linewidth=0.8)

ax = plotTimeSeries(data=trainDF, X="date", y="pr_boosting_timebased", axs=ax, 
                    color="red", title="Число продаж в Эквадоре по датам за 2016 год (Бустинг)",
                    label="Временные переменные", linestyle="--")

ax = plotTimeSeries(data=trainDF, X="date", y="pr_boosting_all", axs=ax, xlabel="Дата",
                    ylabel="Количество продаж", color="green", title="Число продаж в Эквадоре по датам за 2016 год (Бустинг)",
                    label="Все переменные",
                    linestyle="--", savePath=f"{graphsDir}/SalesBoosting.png")





errorsDf = trainDF.query("prediction == 1")
errorsDf_stat = trainDF_new.query("prediction == 1")

stats = (
    errorsCheck(errorsDf["sales"], errorsDf["forecast_manual"])
    .rename(columns={"value":"manual"})
    .merge(errorsCheck(errorsDf["sales"], errorsDf["forecast_SARIMAX_timebased"])
           .rename(columns={"value":"SARIMAX (TimeBased)"}), on="metric")
    .merge(errorsCheck(errorsDf["sales"], errorsDf["forecast_SARIMAX_all"])
           .rename(columns={"value":"SARIMAX (TimeBased&Features)"}), on="metric")
    .merge(errorsCheck(errorsDf["sales"], errorsDf["forecast_naive"])
           .rename(columns={"value":"naive"}), on="metric")
    .merge(errorsCheck(errorsDf["sales"], errorsDf["pr_boosting_timebased"])
           .rename(columns={"value":"Bosting (TimeBased)"}), on="metric")
    .merge(errorsCheck(errorsDf["sales"], errorsDf["pr_boosting_all"])
           .rename(columns={"value":"Boosting (TimeBased&Features)"}), on="metric")
    .merge(errorsCheck(errorsDf_stat["sales"], errorsDf_stat["forecast_manual"])
       .rename(columns={"value":"ARIMA"}), on="metric")
)
stats









